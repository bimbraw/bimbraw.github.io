pub_date	title	venue	excerpt	citation	url_slug	paper_url
2023-01-18	Simultaneous Estimation of Hand Configurations and Finger Joint Angles Using Forearm Ultrasound	IEEE Transactions on Medical Robotics and Bionics	This paper is an extension of our IEEE ICRA 2022 work with a focus on generalizing the classification and angle estimation task to any ML/DL architecture, and doing an extensive analysis of data at different resolutions, acquisition speeds and subjects.	Bimbraw, K., Nycz, C. J., Schueler, M., Zhang, Z., & Zhang, H. K. (2021). "Simultaneous Estimation of Hand Configurations and Finger Joint Angles Using Forearm Ultrasound", <i>IEEE Transactions on Medical Robotics and Bionics</i>, vol. 5, no. 1, pp. 120-132, Feb. 2023, doi: 10.1109/TMRB.2023.3237774.	tmrb-2023	https://ieeexplore.ieee.org/abstract/document/10020174
2022-07-12	Prediction of Metacarpophalangeal joint angles and Classification of Hand configurations based on Ultrasound Imaging of the Forearm	IEEE ICRA 2022	With the advancement in computing and robotics, it is necessary to develop fluent and intuitive methods for interacting with digital systems, AR/VR interfaces, and physical robotic systems. Hand movement recognition is widely used to enable this interaction. Hand configuration classification and Metacarpophalangeal (MCP) joint angle detection are important for a comprehensive reconstruction of the hand motion. Surface electromyography and other technologies have been used for the detection of hand motions. Ultrasound images of the forearm offer a way to visualize the internal physiology of the hand from a musculoskeletal perspective. Recent work has shown that these images can be classified using machine learning to predict various hand configurations. In this paper, we propose a Convolutional Neural Network (CNN) based deep learning pipeline for predicting the MCP joint angles. We supplement our results by using a Support Vector Classifier (SVC) to classify the ultrasound information into several predefined hand configurations based on activities of daily living (ADL). Ultrasound data from the forearm was obtained from 6 subjects who were instructed to move their hands according to predefined hand configurations relevant to ADLs. Motion capture data was acquired as the ground truth for hand movements at different speeds (0.5 Hz, 1 Hz, & 2 Hz) for the index, middle, ring, and pinky fingers. We were able to get promising SVC classification results on a subset of our collected data set. We demonstrated a correspondence between the predicted MCP joint angles and the actual MCP joint angles for the fingers, with an average root mean square error of 7.35 degrees. We implemented a low latency (6.25 - 9.1 Hz) pipeline for the prediction of both MCP joint angles and hand configuration estimation aimed at real-time control of digital devices, AR/VR interfaces, and physical robots.	Bimbraw, K., Nycz, C. J., Schueler, M., Zhang, Z., & Zhang, H. K. (2021). "Prediction of Metacarpophalangeal joint angles and Classification of Hand configurations based on Ultrasound Imaging of the Forearm." <i>2022 International Conference on Robotics and Automation (ICRA)</i>, Philadelphia, PA, USA, 2022, pp. 91-97, doi: 10.1109/ICRA46639.2022.9812287.	icra-2022	https://ieeexplore.ieee.org/abstract/document/9812287
2021-03-24	Tele-operative Robotic Lung Ultrasound Scanning Platform for Triage of COVID-19 Patients	IEEE Robotics and Automation Letters	A cost-effective 2D tele-operative robotic platform for lung ultrasound (LUS), addressing COVID-19 diagnosis challenges and minimizing physical contact. The framework's key contribution is enhancing LUS accessibility and safety, proving successful application in humans, with my primary role being the development of the ultrasound data acquisition software.	Tsumura, R., Hardin, J. W., Bimbraw, K., Odusanya, O. S., Zheng, Y., Hill, J. C., Hoffmann, B., Soboyejo, W., Zhang, H. (2021). "Tele-Operative Low-Cost Robotic Lung Ultrasound Scanning Platform for Triage of COVID-19 Patients" In: <i>IEEE Robotics and Automation Letters, 6</i>(3), 4664-4671.	ral-2021	https://pubmed.ncbi.nlm.nih.gov/34532570/
2020-10-01	Augmented Reality-Based Lung Ultrasound Scanning Guidance	MICCAI ASMUS	Lung ultrasound (LUS) is an established non-invasive imaging method for diagnosing respiratory illnesses. With the rise of SARS-CoV-2 (COVID-19) as a global pandemic, LUS has been used to detect pneumopathy for triaging and monitoring patients who are diagnosis or suspected with COVID-19 infection. While LUS offers a cost-effective, radiation-free, and higher portability compared with chest X-ray and CT, its accessibility is limited due to its user dependency and small number of physicians and sonographers who can perform appropriate scanning and diagnosis. In this paper, we propose a framework of guiding LUS scanning featuring augmented reality, in which the LUS procedure can be guided by projecting the scanning trajectory. To develop such a system, we implement a computer vision-based detection algorithm to classify different regions on human body. The DensePose algorithm is used to obtain a body mesh data for the upper body pictured with a mono-camera. Torso sub-mesh is used to extract and overlay the eight regions corresponding to anterior and lateral chests for LUS guidance. To minimize instability of the DensePose mesh coordinates based on different frontal angles of camera, a machine learning regression algorithm is applied to predict the angle-specific projection model with respect to the chest. ArUco markers are utilized for training the ground truth chest regions to be scanned and another single ArUco marker is used for detecting the center line of the body. The augmented scanning regions are highlighted one by one to guide the scanning path to execute the LUS procedure. We demonstrated the feasibility of guiding the LUS scanning procedure through the combination of augmented reality, computer vision, and machine learning.	Bimbraw, K., Ma, X., Zhang, Z., Zhang, H. (2020). "Augmented Reality-Based Lung Ultrasound Scanning Guidance". In: <i>Medical Ultrasound, and Preterm, Perinatal and Paediatric Image Analysis. ASMUS 2020, PIPPI 2020.</i> Lecture Notes in Computer Science, vol 12437. Springer, Cham.	miccai-2020	https://link.springer.com/chapter/10.1007/978-3-030-60334-2_11
2020-04-01	Towards Sonomyography-Based Real-Time Control of Powered Prosthesis Grasp Synergies	IEEE EMBC 2020	The paper describes the classification of ultrasound information and its mapping onto a soft robotic gripper. In our real-time ultrasound-based control of a soft robotic gripper pipeline, we were able to train a machine learning model to classify 4 hand grasping configurations with an average accuracy percentage of 93%. The paper is a step toward intuitive and robust biosignal-based control methods for robots.	Bimbraw, K., Fox, E., Weinberg, G. and Hammond, F. L. (2020). "Towards Sonomyography-Based Real-Time Control of Powered Prosthesis Grasp Synergies." <i>2020 42nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</i>, Montreal, QC, Canada, 2020, pp. 4753-4757.	embc-2020	https://ieeexplore.ieee.org/document/9176483
